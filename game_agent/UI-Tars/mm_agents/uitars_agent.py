import ast
import base64
import logging
import math
import re
import xml.etree.ElementTree as ET
from io import BytesIO
from typing import Dict, List

import backoff
import numpy as np
from PIL import Image
import openai
from openai import OpenAI



from mm_agents.accessibility_tree_wrap.heuristic_retrieve import (
    filter_nodes,
)
from mm_agents.prompts import (
    UITARS_ACTION_SPACE,
    UITARS_CALL_USR_ACTION_SPACE,
    UITARS_USR_PROMPT_NOTHOUGHT,
    UITARS_USR_PROMPT_THOUGHT,
    UITARS_NORMAL_ACTION_SPACE
)


logger = logging.getLogger("desktopenv.agent")

FINISH_WORD = "finished"
WAIT_WORD = "wait"
ENV_FAIL_WORD = "error_env"
CALL_USER = "call_user"

IMAGE_FACTOR = 28
MIN_PIXELS = 100 * 28 * 28
MAX_PIXELS = 16384 * 28 * 28
MAX_RATIO = 200

pure_text_settings = ["a11y_tree"]



# å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è§£ææ¯ä¸ª action
def parse_action(action_str):
    try:
        # è§£æå­—ç¬¦ä¸²ä¸º AST èŠ‚ç‚¹
        node = ast.parse(action_str, mode='eval')

        # ç¡®ä¿èŠ‚ç‚¹æ˜¯ä¸€ä¸ªè¡¨è¾¾å¼
        if not isinstance(node, ast.Expression):
            raise ValueError("Not an expression")

        # è·å–è¡¨è¾¾å¼çš„ä¸»ä½“
        call = node.body

        # ç¡®ä¿ä¸»ä½“æ˜¯ä¸€ä¸ªå‡½æ•°è°ƒç”¨
        if not isinstance(call, ast.Call):
            raise ValueError("Not a function call")

        # è·å–å‡½æ•°å
        if isinstance(call.func, ast.Name):
            func_name = call.func.id
        elif isinstance(call.func, ast.Attribute):
            func_name = call.func.attr
        else:
            func_name = None

        # è·å–å…³é”®å­—å‚æ•°
        kwargs = {}
        for kw in call.keywords:
            key = kw.arg
            # å¤„ç†ä¸åŒç±»å‹çš„å€¼ï¼Œè¿™é‡Œå‡è®¾éƒ½æ˜¯å¸¸é‡
            if isinstance(kw.value, ast.Constant):
                value = kw.value.value
            elif isinstance(kw.value, ast.Str):  # å…¼å®¹æ—§ç‰ˆæœ¬ Python
                value = kw.value.s
            else:
                value = None
            kwargs[key] = value

        return {
            'function': func_name,
            'args': kwargs
        }

    except Exception as e:
        print(f"Failed to parse action '{action_str}': {e}")
        return None
    
def escape_single_quotes(text):
    # åŒ¹é…æœªè½¬ä¹‰çš„å•å¼•å·ï¼ˆä¸åŒ¹é… \\'ï¼‰
    pattern = r"(?<!\\)'"
    return re.sub(pattern, r"\\'", text)

def round_by_factor(number: int, factor: int) -> int:
    """Returns the closest integer to 'number' that is divisible by 'factor'."""
    return round(number / factor) * factor


def ceil_by_factor(number: int, factor: int) -> int:
    """Returns the smallest integer greater than or equal to 'number' that is divisible by 'factor'."""
    return math.ceil(number / factor) * factor


def floor_by_factor(number: int, factor: int) -> int:
    """Returns the largest integer less than or equal to 'number' that is divisible by 'factor'."""
    return math.floor(number / factor) * factor

def linear_resize(
    height: int, width: int, factor: int = IMAGE_FACTOR, min_pixels: int = MIN_PIXELS, max_pixels: int = MAX_PIXELS
) -> tuple[int, int]:
    if width * height > max_pixels:
        """
        å¦‚æœå›¾ç‰‡è¶…è¿‡/ä½äºåƒç´ é™åˆ¶ï¼Œåˆ™è®¡ç®—ä¸€ä¸ªç¼©æ”¾å› å­resize_factorï¼Œä½¿å›¾ç‰‡çš„åƒç´ æ•°ç¼©å°åˆ°ç­‰äºæˆ–å°äºmax_pixelsã€‚è¿™ä¸ªç¼©æ”¾å› å­æ˜¯é€šè¿‡å¼€å¹³æ–¹æ ¹è®¡ç®—çš„ï¼Œç¡®ä¿çºµæ¨ªæ¯”ä¿æŒä¸å˜,è¿™æ ·åŸå§‹çš„ç›¸å¯¹åæ ‡å¯ä»¥ä¸ç»è½¬æ¢ç›´æ¥å¤ç”¨
        """
        resize_factor = math.sqrt(max_pixels / (width * height))
        width, height = int(width * resize_factor), int(height * resize_factor)
    if width * height < min_pixels:
        resize_factor = math.sqrt(min_pixels / (width * height))
        width, height = math.ceil(width * resize_factor), math.ceil(height * resize_factor)

    return height, width 

def smart_resize(
    height: int, width: int, factor: int = IMAGE_FACTOR, min_pixels: int = MIN_PIXELS, max_pixels: int = MAX_PIXELS
) -> tuple[int, int]:
    """
    Rescales the image so that the following conditions are met:

    1. Both dimensions (height and width) are divisible by 'factor'.

    2. The total number of pixels is within the range ['min_pixels', 'max_pixels'].

    3. The aspect ratio of the image is maintained as closely as possible.
    """
    if max(height, width) / min(height, width) > MAX_RATIO:
        raise ValueError(
            f"absolute aspect ratio must be smaller than {MAX_RATIO}, got {max(height, width) / min(height, width)}"
        )
    h_bar = max(factor, round_by_factor(height, factor))
    w_bar = max(factor, round_by_factor(width, factor))
    if h_bar * w_bar > max_pixels:
        beta = math.sqrt((height * width) / max_pixels)
        h_bar = floor_by_factor(height / beta, factor)
        w_bar = floor_by_factor(width / beta, factor)
    elif h_bar * w_bar < min_pixels:
        beta = math.sqrt(min_pixels / (height * width))
        h_bar = ceil_by_factor(height * beta, factor)
        w_bar = ceil_by_factor(width * beta, factor)
    return h_bar, w_bar

def parse_action_to_structure_output(text, factor, origin_resized_height, origin_resized_width, model_type, max_pixels=16384*28*28, min_pixels=100*28*28):
    text = text.strip()
    if model_type == "qwen25vl":
        smart_resize_height, smart_resize_width = smart_resize(origin_resized_height, origin_resized_width, factor=IMAGE_FACTOR, min_pixels=min_pixels, max_pixels=max_pixels)

    # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… Action å­—ç¬¦ä¸²
    if text.startswith("Thought:"):
        thought_pattern = r"Thought: (.+?)(?=\s*Action:|$)"
        thought_hint = "Thought: "
    elif text.startswith("Reflection:"):
        thought_pattern = r"Reflection: (.+?)Action_Summary: (.+?)(?=\s*Action:|$)"
        thought_hint = "Reflection: "
    elif text.startswith("Action_Summary:"):
        thought_pattern = r"Action_Summary: (.+?)(?=\s*Action:|$)"
        thought_hint = "Action_Summary: "
    else:
        thought_pattern = r"Thought: (.+?)(?=\s*Action:|$)"
        thought_hint = "Thought: "
    reflection, thought = None, None
    thought_match = re.search(thought_pattern, text, re.DOTALL)
    if thought_match:
        if len(thought_match.groups()) == 1:
            thought = thought_match.group(1).strip()
        elif len(thought_match.groups()) == 2:
            thought = thought_match.group(2).strip()
            reflection = thought_match.group(1).strip()
    assert "Action:" in text
    action_str = text.split("Action:")[-1]

    tmp_all_action = action_str.split("\n\n")
    all_action = []
    for action_str in tmp_all_action:
        if "type(content" in action_str:
            # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… content ä¸­çš„å­—ç¬¦ä¸²å¹¶è½¬ä¹‰å•å¼•å·
            def escape_quotes(match):
                content = match.group(1)  # è·å– content çš„å€¼
                return content

            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œæ›¿æ¢
            pattern = r"type\(content='(.*?)'\)"  # åŒ¹é… type(content='...')
            content = re.sub(pattern, escape_quotes, action_str)

            # å¤„ç†å­—ç¬¦ä¸²
            action_str = escape_single_quotes(content)
            action_str = "type(content='" + action_str + "')"
        all_action.append(action_str)

    parsed_actions = [parse_action(action.replace("\n","\\n").lstrip()) for action in all_action]
    actions = []
    for action_instance, raw_str in zip(parsed_actions, all_action):
        if action_instance == None:
            print(f"Action can't parse: {raw_str}")
            raise ValueError(f"Action can't parse: {raw_str}") 
        action_type = action_instance["function"]
        params = action_instance["args"]

        # import pdb; pdb.set_trace()
        action_inputs = {}
        for param_name, param in params.items():
            if param == "": continue
            param = param.lstrip()  # å»æ‰å¼•å·å’Œå¤šä½™çš„ç©ºæ ¼
            # å¤„ç†start_boxæˆ–è€…end_boxå‚æ•°æ ¼å¼ '<bbox>x1 y1 x2 y2</bbox>'
            action_inputs[param_name.strip()] = param
            
            if "start_box" in param_name or "end_box" in param_name:
                ori_box = param
                # Remove parentheses and split the string by commas
                numbers = ori_box.replace("(", "").replace(")", "").split(",")

                # Convert to float and scale by 1000
                # Qwen2.5vl output absolute coordinates, qwen2vl output relative coordinates
                if model_type == "qwen25vl":
                    float_numbers = []
                    for num_idx, num in enumerate(numbers):
                        num = float(num)
                        if (num_idx + 1) % 2 == 0:
                            float_numbers.append(float(num/smart_resize_height))
                        else:
                            float_numbers.append(float(num/smart_resize_width))
                else:
                    float_numbers = [float(num) / factor for num in numbers]

                if len(float_numbers) == 2:
                    float_numbers = [float_numbers[0], float_numbers[1], float_numbers[0], float_numbers[1]]
                action_inputs[param_name.strip()] = str(float_numbers)

        # import pdb; pdb.set_trace()
        actions.append({
            "reflection": reflection,
            "thought": thought,
            "action_type": action_type,
            "action_inputs": action_inputs,
            "text": text
        })
    return actions

def parsing_response_to_pyautogui_code(responses, image_height: int, image_width:int, input_swap:bool=True) -> str:
    '''
    å°†Mæ¨¡å‹çš„è¾“å‡ºè§£æä¸ºOSWorldä¸­çš„actionï¼Œç”Ÿæˆpyautoguiä»£ç å­—ç¬¦ä¸²
    å‚æ•°:
        response: åŒ…å«æ¨¡å‹è¾“å‡ºçš„å­—å…¸ï¼Œç»“æ„ç±»ä¼¼äºï¼š
        {
            "action_type": "hotkey",
            "action_inputs": {
                "hotkey": "v ctrl",
                "start_box": None,
                "end_box": None
            }
        }
    è¿”å›:
        ç”Ÿæˆçš„pyautoguiä»£ç å­—ç¬¦ä¸²
    '''

    pyautogui_code = f"import pyautogui\nimport time\n"
    if isinstance(responses, dict):
        responses = [responses]
    for response_id, response in enumerate(responses):
        if "observation" in response:
            observation = response["observation"]
        else:
            observation = ""

        if "thought" in response:
            thought = response["thought"]
        else:
            thought = ""
        
        if response_id == 0:
            pyautogui_code += f"'''\nObservation:\n{observation}\n\nThought:\n{thought}\n'''\n"
        else:
            pyautogui_code += f"\ntime.sleep(1)\n"

        action_dict = response
        action_type = action_dict.get("action_type")
        action_inputs = action_dict.get("action_inputs", {})
        
        if action_type == "hotkey":
            # Parsing hotkey action
            if "key" in action_inputs:
                hotkey = action_inputs.get("key", "")
            else:
                hotkey = action_inputs.get("hotkey", "")

            if hotkey == "arrowleft":
                hotkey = "left"

            elif hotkey == "arrowright":
                hotkey = "right"
            
            elif hotkey == "arrowup":
                hotkey = "up"
            
            elif hotkey == "arrowdown":
                hotkey = "down"

            if hotkey:
                # Handle other hotkeys
                keys = hotkey.split()  # Split the keys by space
                convert_keys = []
                for key in keys:
                    if key == "space":
                        key = ' '
                    convert_keys.append(key)
                pyautogui_code += f"\npyautogui.hotkey({', '.join([repr(k) for k in convert_keys])})"
        
        elif action_type == "press":
            # Parsing press action
            if "key" in action_inputs:
                key_to_press = action_inputs.get("key", "")
            else:
                key_to_press = action_inputs.get("press", "")

            if hotkey == "arrowleft":
                hotkey = "left"

            elif hotkey == "arrowright":
                hotkey = "right"
            
            elif hotkey == "arrowup":
                hotkey = "up"
            
            elif hotkey == "arrowdown":
                hotkey = "down"
            
            elif hotkey == "space":
                hotkey = " "
                
            if key_to_press:
                # Simulate pressing a single key
                pyautogui_code += f"\npyautogui.press({repr(key_to_press)})"
            
        elif action_type == "keyup":
            key_to_up = action_inputs.get("key", "")
            pyautogui_code += f"\npyautogui.keyUp({repr(key_to_up)})"
        
        elif action_type == "keydown":
            key_to_down = action_inputs.get("key", "")
            pyautogui_code += f"\npyautogui.keyDown({repr(key_to_down)})"

        elif action_type == "type":
            # Parsing typing action using clipboard
            content = action_inputs.get("content", "")
            content = escape_single_quotes(content)
            stripped_content = content
            if content.endswith("\n") or content.endswith("\\n"):
                stripped_content = stripped_content.rstrip("\\n").rstrip("\n")
            if content:
                if input_swap:
                    pyautogui_code += f"\nimport pyperclip"
                    pyautogui_code += f"\npyperclip.copy('{stripped_content}')"
                    pyautogui_code += f"\npyautogui.hotkey('ctrl', 'v')"
                    pyautogui_code += f"\ntime.sleep(0.5)\n"
                    if content.endswith("\n") or content.endswith("\\n"):
                        pyautogui_code += f"\npyautogui.press('enter')"
                else:
                    pyautogui_code += f"\npyautogui.write('{stripped_content}', interval=0.1)"
                    pyautogui_code += f"\ntime.sleep(0.5)\n"
                    if content.endswith("\n") or content.endswith("\\n"):
                        pyautogui_code += f"\npyautogui.press('enter')"

        
        elif action_type in ["drag", "select"]:
            # Parsing drag or select action based on start and end_boxes
            start_box = action_inputs.get("start_box")
            end_box = action_inputs.get("end_box")
            if start_box and end_box:
                x1, y1, x2, y2 = eval(start_box)  # Assuming box is in [x1, y1, x2, y2]
                sx = round(float((x1 + x2) / 2) * image_width, 3)
                sy = round(float((y1 + y2) / 2) * image_height, 3)
                x1, y1, x2, y2 = eval(end_box)  # Assuming box is in [x1, y1, x2, y2]
                ex = round(float((x1 + x2) / 2) * image_width, 3)
                ey = round(float((y1 + y2) / 2) * image_height, 3)
                pyautogui_code += (
                    f"\npyautogui.moveTo({sx}, {sy})\n"
                    f"pyautogui.dragTo({ex}, {ey}, duration=1.0, button='left')\n"
                )

        elif action_type == "scroll":
            # Parsing scroll action
            start_box = action_inputs.get("start_box")
            if start_box:
                x1, y1, x2, y2 = eval(start_box)  # Assuming box is in [x1, y1, x2, y2]
                x = round(float((x1 + x2) / 2) * image_width, 3)
                y = round(float((y1 + y2) / 2) * image_height, 3)
                
                # # å…ˆç‚¹å¯¹åº”åŒºåŸŸï¼Œå†æ»šåŠ¨
                # pyautogui_code += f"\npyautogui.click({x}, {y}, button='left')"
            else:
                x = None
                y = None
            direction = action_inputs.get("direction", "")
            
            if x == None:
                if "up" in direction.lower():
                    pyautogui_code += f"\npyautogui.scroll(5)"
                elif "down" in direction.lower():
                    pyautogui_code += f"\npyautogui.scroll(-5)"
            else:
                if "up" in direction.lower():
                    pyautogui_code += f"\npyautogui.scroll(5, x={x}, y={y})"
                elif "down" in direction.lower():
                    pyautogui_code += f"\npyautogui.scroll(-5, x={x}, y={y})"

        elif action_type in ["click", "left_single", "left_double", "right_single", "hover"]:
            # Parsing mouse click actions
            start_box = action_inputs.get("start_box")
            start_box = str(start_box)
            if start_box:
                start_box = eval(start_box)
                if len(start_box) == 4:
                    x1, y1, x2, y2 = start_box  # Assuming box is in [x1, y1, x2, y2]
                elif len(start_box) == 2:
                    x1, y1 = start_box
                    x2 = x1
                    y2 = y1
                x = round(float((x1 + x2) / 2) * image_width, 3)
                y = round(float((y1 + y2) / 2) * image_height, 3)
                if action_type == "left_single" or action_type == "click":
                    pyautogui_code += f"\npyautogui.click({x}, {y}, button='left')"
                elif action_type == "left_double":
                    pyautogui_code += f"\npyautogui.doubleClick({x}, {y}, button='left')"
                elif action_type == "right_single":
                    pyautogui_code += f"\npyautogui.click({x}, {y}, button='right')"
                elif action_type == "hover":
                    pyautogui_code += f"\npyautogui.moveTo({x}, {y})"
        
        elif action_type in ["finished"]:
            pyautogui_code = f"DONE"
        
        else:
            pyautogui_code += f"\n# Unrecognized action type: {action_type}"

    return pyautogui_code

def add_box_token(input_string):
    # Step 1: Split the string into individual actions
    if "Action: " in input_string and "start_box=" in input_string:
        suffix = input_string.split("Action: ")[0] + "Action: "
        actions = input_string.split("Action: ")[1:]
        processed_actions = []
        for action in actions:
            action = action.strip()
            # Step 2: Extract coordinates (start_box or end_box) using regex
            coordinates = re.findall(r"(start_box|end_box)='\((\d+),\s*(\d+)\)'", action)
            
            updated_action = action  # Start with the original action
            for coord_type, x, y in coordinates:
                # Convert x and y to integers
                updated_action = updated_action.replace(f"{coord_type}='({x},{y})'", f"{coord_type}='<|box_start|>({x},{y})<|box_end|>'")
            processed_actions.append(updated_action)
        
        # Step 5: Reconstruct the final string
        final_string = suffix + "\n\n".join(processed_actions)
    else:
        final_string = input_string
    return final_string

def pil_to_base64(image):
    buffer = BytesIO()
    image.save(buffer, format="PNG")  # ä½ å¯ä»¥æ”¹æˆ "JPEG" ç­‰æ ¼å¼
    return base64.b64encode(buffer.getvalue()).decode("utf-8")

def linearize_accessibility_tree(accessibility_tree, platform="ubuntu"):

    if platform == "ubuntu":
        _attributes_ns = attributes_ns_ubuntu
        _state_ns = state_ns_ubuntu
        _component_ns = component_ns_ubuntu
        _value_ns = value_ns_ubuntu
    elif platform == "windows":
        _attributes_ns = attributes_ns_windows
        _state_ns = state_ns_windows
        _component_ns = component_ns_windows
        _value_ns = value_ns_windows
    else:
        raise ValueError("Invalid platform, must be 'ubuntu' or 'windows'")

    filtered_nodes = filter_nodes(ET.fromstring(accessibility_tree), platform)
    linearized_accessibility_tree = [
        "tag\tname\ttext\tclass\tdescription\tposition (top-left x&y)\tsize (w&h)"
    ]

    # Linearize the accessibility tree nodes into a table format
    for node in filtered_nodes:
        if node.text:
            text = (
                node.text
                if '"' not in node.text
                else '"{:}"'.format(node.text.replace('"', '""'))
            )

        elif node.get("{{{:}}}class".format(class_ns_windows), "").endswith(
            "EditWrapper"
        ) and node.get("{{{:}}}value".format(_value_ns)):
            node_text = node.get("{{{:}}}value".format(_value_ns), "")
            text = (
                node_text
                if '"' not in node_text
                else '"{:}"'.format(node_text.replace('"', '""'))
            )
        else:
            text = '""'

        linearized_accessibility_tree.append(
            "{:}\t{:}\t{:}\t{:}\t{:}\t{:}\t{:}".format(
                node.tag,
                node.get("name", ""),
                text,
                (
                    node.get("{{{:}}}class".format(_attributes_ns), "")
                    if platform == "ubuntu"
                    else node.get("{{{:}}}class".format(class_ns_windows), "")
                ),
                node.get("{{{:}}}description".format(_attributes_ns), ""),
                node.get("{{{:}}}screencoord".format(_component_ns), ""),
                node.get("{{{:}}}size".format(_component_ns), ""),
            )
        )

    return "\n".join(linearized_accessibility_tree)

def trim_accessibility_tree(linearized_accessibility_tree, max_tokens):
    # enc = tiktoken.encoding_for_model("gpt-4")
    # tokens = enc.encode(linearized_accessibility_tree)
    # if len(tokens) > max_tokens:
    #     linearized_accessibility_tree = enc.decode(tokens[:max_tokens])
    #     linearized_accessibility_tree += "[...]\n"
    return linearized_accessibility_tree

class UITARSAgent:
    FINISH_WORD = "finished"
    WAIT_WORD = "wait"
    ENV_FAIL_WORD = "error_env"
    CALL_USER = "call_user"

    def __init__(self, action_space="pyautogui", observation_type="screenshot", max_trajectory_length=50,
                 a11y_tree_max_tokens=10000, model_type="qwen25vl", runtime_conf: dict = None):
        if runtime_conf is None:
            runtime_conf = {
                "infer_mode": "qwen25vl_normal",
                "prompt_style": "qwen25vl_normal",
                "input_swap": True,
                "language": "English",
                "history_n": 10,
                "max_pixels": 16384 * 28 * 28,
                "min_pixels": 100 * 28 * 28,
                "callusr_tolerance": 3,
                "temperature": 0.0,
                "top_k": -1,
                "top_p": 0.9,
                "max_tokens": 1000
            }

        self.action_space = action_space
        self.observation_type = observation_type
        self.max_trajectory_length = max_trajectory_length
        self.a11y_tree_max_tokens = a11y_tree_max_tokens
        self.model_type = model_type
        self.runtime_conf = runtime_conf
        self.vlm = OpenAI(base_url="http://your_url/v1", api_key="empty")
        self.temperature = runtime_conf["temperature"]
        self.top_k = runtime_conf["top_k"]
        self.top_p = runtime_conf["top_p"]
        self.max_tokens = runtime_conf["max_tokens"]
        self.infer_mode = runtime_conf["infer_mode"]
        self.prompt_style = runtime_conf["prompt_style"]
        self.input_swap = runtime_conf["input_swap"]
        self.language = runtime_conf["language"]
        self.max_pixels = runtime_conf["max_pixels"]
        self.min_pixels = runtime_conf["min_pixels"]
        self.callusr_tolerance = runtime_conf["callusr_tolerance"]

        self.thoughts = []
        self.actions = []
        self.observations = []
        self.history_images = []
        self.history_responses = []

        self.prompt_action_space = UITARS_ACTION_SPACE
        self.action_parse_res_factor = 1000

        if self.infer_mode == "qwen2vl_user":
            self.prompt_action_space = UITARS_CALL_USR_ACTION_SPACE
        elif self.infer_mode == "qwen25vl_normal":
            self.prompt_action_space = UITARS_NORMAL_ACTION_SPACE

        if self.prompt_style in ["qwen2vl_user", "qwen25vl_normal"]:
            self.prompt_template = UITARS_USR_PROMPT_THOUGHT
        elif self.prompt_style == "qwen2vl_no_thought":
            self.prompt_template = UITARS_USR_PROMPT_NOTHOUGHT

        self.history_n = self.runtime_conf.get("history_n", 5)
        self.cur_callusr_count = 0

    def predict(self, instruction: str, obs: Dict, last_action_after_obs: Dict = None) -> List:
        print("âœ… [predict] ì‹œì‘")

        if len(self.observations) > self.max_trajectory_length:
            _observations = self.observations[-self.max_trajectory_length:]
            _actions = self.actions[-self.max_trajectory_length:]
            _thoughts = self.thoughts[-self.max_trajectory_length:]
        else:
            _observations = self.observations
            _actions = self.actions
            _thoughts = self.thoughts

        self.history_images.append(obs["screenshot"])

        if self.observation_type in ["screenshot", "screenshot_a11y_tree"]:
            base64_image = obs["screenshot"]
            try:
                linearized_accessibility_tree = (
                    linearize_accessibility_tree(obs["accessibility_tree"], self.platform)
                    if self.observation_type == "screenshot_a11y_tree" else None
                )
            except Exception as e:
                print(f"â— a11y tree linearization ì‹¤íŒ¨: {e}")
                linearized_accessibility_tree = None

            if linearized_accessibility_tree:
                linearized_accessibility_tree = trim_accessibility_tree(
                    linearized_accessibility_tree, self.a11y_tree_max_tokens
                )

            self.observations.append({
                "screenshot": base64_image,
                "accessibility_tree": linearized_accessibility_tree
            })
        else:
            raise ValueError("Invalid observation_type type: " + self.observation_type)

        if self.infer_mode in ["qwen2vl_user", "qwen25vl_normal"]:
            user_prompt = self.prompt_template.format(
                instruction=instruction,
                action_space=self.prompt_action_space,
                language=self.language
            )
        else:
            user_prompt = self.prompt_template.format(instruction=instruction)

        if len(self.history_images) > self.history_n:
            self.history_images = self.history_images[-self.history_n:]

        images = []
        for img_data in self.history_images:
            try:
                img = Image.open(img_data)
                if img.width * img.height > self.max_pixels:
                    resize_factor = math.sqrt(self.max_pixels / (img.width * img.height))
                    img = img.resize((int(img.width * resize_factor), int(img.height * resize_factor)))
                elif img.width * img.height < self.min_pixels:
                    resize_factor = math.sqrt(self.min_pixels / (img.width * img.height))
                    img = img.resize((math.ceil(img.width * resize_factor), math.ceil(img.height * resize_factor)))
                if img.mode != "RGB":
                    img = img.convert("RGB")
                images.append(img)
            except Exception as e:
                print(f"â— ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

        messages = [
            {"role": "system", "content": [{"type": "text", "text": "You are a helpful assistant."}]},
            {"role": "user", "content": [{"type": "text", "text": user_prompt}]}
        ]

        for i, history_response in enumerate(self.history_responses[-self.history_n:]):
            encoded = pil_to_base64(images[i])
            messages.append({"role": "user", "content": [{"type": "image_url", "image_url": {"url": f"data:image/png;base64,{encoded}"}}]})
            messages.append({"role": "assistant", "content": [add_box_token(history_response)]})

        cur_image = images[-1]
        encoded_string = pil_to_base64(cur_image)
        messages.append({"role": "user", "content": [{"type": "image_url", "image_url": {"url": f"data:image/png;base64,{encoded_string}"}}]})

        try_times = 3
        origin_resized_height = cur_image.height
        origin_resized_width = cur_image.width

        response = None
        while try_times > 0:
            try:
                response = self.vlm.chat.completions.create(
                    model="ui-tars",
                    messages=messages,
                    frequency_penalty=1,
                    max_tokens=self.max_tokens,
                    temperature=self.temperature
                )
                prediction = response.choices[0].message.content.strip()
                break
            except Exception as e:
                print(f"â— ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                try_times -= 1

        if response is None:
            return "client error", ["DONE"]

        try:
            parsed_responses = parse_action_to_structure_output(
                prediction,
                self.action_parse_res_factor,
                origin_resized_height,
                origin_resized_width,
                self.model_type,
                self.max_pixels,
                self.min_pixels
            )
        except Exception as e:
            print(f"â— íŒŒì‹± ì‹¤íŒ¨: {e}\në‚´ìš©: {prediction}")
            return f"Parsing error: {e}", ["DONE"]

        actions = []
        obs_image_height = cur_image.height
        obs_image_width = cur_image.width

        for parsed_response in parsed_responses:
            atype = parsed_response.get("action_type")
            if atype == self.FINISH_WORD:
                self.actions.append(actions)
                return prediction, ["DONE"]
            elif atype == self.WAIT_WORD:
                self.actions.append(actions)
                return prediction, ["WAIT"]
            elif atype == self.ENV_FAIL_WORD:
                self.actions.append(actions)
                return prediction, ["FAIL"]
            elif atype == self.CALL_USER:
                if self.cur_callusr_count < self.callusr_tolerance:
                    self.cur_callusr_count += 1
                    return prediction, ["WAIT"]
                else:
                    return prediction, ["FAIL"]

            pyautogui_code = parsing_response_to_pyautogui_code(
                parsed_response,
                obs_image_height,
                obs_image_width,
                self.input_swap
            )
            actions.append(pyautogui_code)

        self.actions.append(actions)

        total_action_count = sum(len(action_list) for action_list in self.actions)
        print(f"ğŸ”¢ ëˆ„ì  ì•¡ì…˜ ê°œìˆ˜: {total_action_count}")
        if total_action_count >= 1000:
            print(f"âœ… [predict] Action count {total_action_count} reached. Ending with DONE.")
            return prediction, ["DONE"]

        return prediction, actions

    @backoff.on_exception(
        backoff.constant,
        (openai.RateLimitError, openai.BadRequestError, openai.InternalServerError),
        interval=20,
        max_tries=10,
    )
    def reset(self, runtime_logger):
        self.thoughts = []
        self.actions = []
        self.observations = []
        self.history_images = []
        self.history_responses = []
